# End-to-End Data Engineering Project: dbt, Snowflake & Apache Airflow
## Overview
This project is a complete data engineering pipeline using dbt (Data Build Tool), Snowflake (Data Warehouse), and Apache Airflow (Orchestration Tool). It covers data ingestion, transformation, and scheduling in a structured and scalable manner.

## Tech Stack
- **dbt Core** – For data transformation and modeling  
- **Snowflake** – Cloud-based data warehouse  
- **Apache Airflow** – Workflow automation and orchestration  
- **Python** – Scripting and automation  
Run dbt Models
```sh
dbt run
dbt test  # To validate data integrity
```

Start Apache Airflow
```sh
airflow standalone  # Starts the UI & Scheduler
```
![snowflake](https://github.com/user-attachments/assets/c511c962-9600-4803-8d70-180d1c64af90)
![dbt_run_pipeline-graph](https://github.com/user-attachments/assets/7b24a822-647c-40a3-9275-305f65941869)
![dbt](https://github.com/user-attachments/assets/cd8183bc-af7f-4970-97f4-d5ca43f9686c)

![Airflowpng](https://github.com/user-attachments/assets/504f7830-c5bc-4044-bbf4-00074fe8f892)
